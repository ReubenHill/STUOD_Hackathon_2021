{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Climate Science Tutorial with Python\n",
    "\n",
    "In this three-part tutorial series, we introduce some of the basic tools that are used in machine learning that may be useful for the tasks presented in _Challenge 3: Machine learning with climate and weather data_. \n",
    "\n",
    "__Note:__ this is a beginner's tutorial and aimed at students/researchers who are new to machine learning or are just getting started.\n",
    "\n",
    "## Introduction\n",
    "Broadly speaking, machine learning is a sub-field of artificial intelligence and statistics that deals with the automatic extraction of patterns (i.e. learning) from large amounts of data. We may categorize the types of learning as either __supervised__ or __unsupervised__ depending on whether the data we are given is labelled or not (there are also other types of learning such as _reinforcement learning_ that deals with learning from past mistakes, and *semi-supervised learning*, where only a subset of the data is labelled).\n",
    "\n",
    "![Different types of machine learning](../images/ml-types.png)\n",
    "\n",
    "In this tutorial, we'll only be concerned with supervised learning tasks, so our data will be *labelled*, which means that it consists of a pair $(X_n, y_n)_{n=1}^N$ of __inputs__ $X_n$ and __labels__ $y_n$.\n",
    "\n",
    "__Example:__ The inputs $X_n$ are images of chihuahuas and muffins, and the labels $y_n$ are the tags 'chihuahua' or 'muffin' (yes you read that right).\n",
    "\n",
    "![chihuahuas vs muffins](../images/dogmuffin.png)\n",
    "\n",
    "The primary task of supervised learning is to find a mapping $f : X \\mapsto y$ that best represents the relationship between the inputs and labels, which, in ML lingo, is called **fitting the data**. Depending on whether the labels $y_n$ take values in a discrete or continuous set, the set of techniques used to fit the data are slightly different. The corresponding tasks also have different names: the former is called __classification__ and the latter is called **regression**.\n",
    "\n",
    "In this tutorial, we will cover both classification and regression tasks in detail using synthetic data, before moving on to real datasets. The outline of the tutorial is as follows:\n",
    "1. Classification with the Lorenz 63 system\n",
    "2. Regression with the Lorenz 96 system\n",
    "3. Handling real world datasets with `pandas`\n",
    "\n",
    "\n",
    "## Part 1: Classification with the Lorenz 63 system\n",
    "\n",
    "In the first part of this tutorial, we are going to introduce the basics of *statistical classification* using the famous Lorenz 63 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "currentdir = os.path.dirname(os.path.realpath('__file__'))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.append(parentdir)\n",
    "from datasets.lorenz import Lorenz63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lorenz 63 model (often shortened as L63) is given by the following three-dimensional set of nonlinear ODEs, introduced by the meteorologist Edward Lorenz in 1963 as a toy model for atmospheric convection:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{dx}{dt} &= \\sigma (y - x), \\\\\n",
    "\\frac{dy}{dt} &= x(\\rho - z) - y, \\\\\n",
    "\\frac{dz}{dt} &= xy - \\beta z.\n",
    "\\end{align*}\n",
    "\n",
    "This model is famous for jump-starting the field of _chaos theory_ that has since become important in the physical sciences and especially in meteorology, where it was instrumental in showing the impossibility of forecasting the weather far into the future.\n",
    "\n",
    "This model has three parameters $(\\sigma, \\rho, \\beta)$, which are usually set at $\\sigma = 10, \\rho = 28$ and $\\beta = 8/3$, at which the system exhibits chaotic behaviour.\n",
    "\n",
    "Let's integrate and visualise the L63 system with these parameters and see what we get..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot the Lorenz attractor\n",
    "tmax = 100\n",
    "num_samples = 10000\n",
    "dt = tmax/num_samples # 0.01\n",
    "X0 = np.array([0, 1, 1.05]) # Initial condition\n",
    "\n",
    "L63 = Lorenz63(tmax, X0, num_samples)\n",
    "L63.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The famous butterfly! This butterfly is called the *Lorenz attractor*.\n",
    "Now let's say that for any point $X_0 = (x_0,y_0,z_0)$ on the Lorenz attractor at time $t=0$, we want to predict where it will be at time $t=1$. Further, let's assume that we have no knowledge of the L63 equations and only have noisy measurements of its $(x,y,z)$-coordinates. \n",
    "Is this possible?\n",
    "\n",
    "Since the L63 system is chaotic, predicting the exact position is going to be impossible so we will reformulate this problem to make it a bit more tractable:\n",
    "\n",
    "#### Challenge: Given any point $X_0$ on the Lorenz attractor, determine which wing of the butterfly (left or right) it will end up in after time $t=1$.\n",
    "\n",
    "Here, we define the right/left butterfly wing of the Lorenz attractor according to whether the $x$-component of $X$ is positive or negative respectively.\n",
    "\n",
    "\\begin{align*}\n",
    "x \\leq 0 & \\quad \\Rightarrow \\quad X \\in \\text{left butterfly wing (class 0)} \\\\\n",
    "x > 0 & \\quad \\Rightarrow \\quad X \\in \\text{right butterfly wing (class 1)}\n",
    "\\end{align*}\n",
    "\n",
    "You should realise that this is a _classification task_ since the labels 'left butterfly wing' and 'right butterfly wing' form a discrete set.\n",
    "\n",
    "To proceed, we first construct the function `end_up_on_which_wing` that takes in a L63 trajectory $(X_{t_n})_{n=1}^N$ and outputs a sequence of labels (0 or 1) depending on whether the future point $X_{t_n + \\Delta t}$ belongs to the left or right butterfly wing respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_up_on_which_wing(X: np.array, lag: int) -> np.array:\n",
    "    \"\"\" Returns array of 0s and 1s corresponding to whether a point on the attractor lands on the\n",
    "        left or right wing respectively after n = lag timesteps\n",
    "        Args:\n",
    "            X: array of shape (num_samples, 3)\n",
    "        \n",
    "        Output: array of shape (num_samples - lag, )\n",
    "    \"\"\"\n",
    "    x, y, z = X.transpose()\n",
    "    return (x[lag:] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this to generate the sequence of *labels* for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfinal = 1.\n",
    "tsteps = int(tfinal/dt)\n",
    "\n",
    "X = L63.trajectory\n",
    "N = L63.num_samples\n",
    "\n",
    "# Get sequence of 0/1s depending on whether each point X ends up on the left/right wing after time t=1\n",
    "lag = tsteps\n",
    "labels = end_up_on_which_wing(X, lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's color-code the points on the attractor to help us visualise the task: orange if the point ends up on the left butterfly wing after time $t=1$ and blue if it ends up on the right butterfly wing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices corresponding to left/right wing after time t=1\n",
    "left_indices = np.argwhere(labels == 0)\n",
    "right_indices = np.argwhere(labels == 1)\n",
    "\n",
    "# Extract subset of X that lies on the left/right wing after time t=1\n",
    "eventually_left_X = X[left_indices].squeeze()\n",
    "eventually_right_X = X[right_indices].squeeze()\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = plt.gca()\n",
    "left_x, left_z = eventually_left_X[:,0], eventually_left_X[:,2]\n",
    "right_x, right_z = eventually_right_X[:,0], eventually_right_X[:,2]\n",
    "plt.plot(left_x, left_z, 'o', color='tab:orange', ms=2, alpha=0.6, label=f'left at t={int(tfinal)}')\n",
    "plt.plot(right_x, right_z, 'o', color='tab:blue', ms=2, alpha=0.6, label=f'right at t={int(tfinal)}')\n",
    "ax.set_xlabel('x', fontsize=15)\n",
    "ax.set_ylabel('z', fontsize=15)\n",
    "ax.set_title(f't = 0', fontsize=15)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this that our objective is to learn a function $f$ that classifies points on the attractor into the orange or blue regions.\n",
    "\n",
    "$$f : \\text{points on attractor} \\rightarrow \\{\\text{orange region}, \\text{blue region}\\}.$$\n",
    "\n",
    "We now construct our labelled dataset $(X, y)$ where the inputs $X$ are noisy observations (here, noise is assumed to be i.i.d. Gaussian) of points on the Lorenz attractor and the labels $y$ are the corresponding 0/1 class, depending on whether it belongs to the orange or blue region above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise level\n",
    "sigma = 0.1\n",
    "\n",
    "# i.i.d. Gaussian noise\n",
    "noise = sigma * np.random.randn(N-lag, 3)\n",
    "\n",
    "# Dataset for our task\n",
    "X = X[:N-lag] + noise\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we will explore three common methods that can be used to fit the data $(X, y)$, namely, logistic regression, decision trees classification and neural network classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at *logistic regression*, where the idea is to model the *probabilities* of an input $X \\in \\mathbb{R}^m$ belonging to classes $1, \\ldots, n$,\n",
    "\n",
    "$$f_i(X) = \\mathbb{P}(X \\in \\text{Class } i), \\quad i = 1, \\ldots, n,$$\n",
    "\n",
    "using a so-called *logistic model*, and then classifying based on the predicted probabilities: If $j = \\text{argmax}_i f_i(X)$, then we predict that $X$ belongs to class $j$. In general, logistic models take the form:\n",
    "\n",
    "$$f(X) = \\sigma(\\text{logit}(X)),$$\n",
    "\n",
    "where $\\sigma$ is the __sigmoid function__ in the binary classification setting (i.e. there are only two classes $0$ or $1$) or the __softmax function__ in the multi-class setting, which squashes real numbers to take values between $0$ and $1$,\n",
    "\n",
    "![Sigmoid graph](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5152157%2F366ce717ad74dfe967a940d9a2161f9b%2Fmain-qimg-6b67bea3311c3429bfb34b6b1737fe0c.jpg?generation=1605711218576073&alt=media)\n",
    "\n",
    "and the _logit_ is the raw model output before squashing via sigmoid/softmax, usually assumed to be linear in the inputs:\n",
    "\n",
    "$$\\text{logit}(X) = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_m X_m.$$\n",
    "\n",
    "Here, $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\ldots, \\beta_m)$ are the parameters of the logistic model, that is to be learned during training.\n",
    "\n",
    "To simplify our discussion on how to fit our model (i.e. finding the optimal $\\boldsymbol{\\beta}$), we'll only discuss the binary classification setting, but the idea can be extended easily to the multi-class setting. Note that for binary classification, we only need to model the probability of $X$ belonging to class $1$, as the probability of belonging to class $0$ can be computed by simply subtracting from $1$, i.e.,\n",
    "\n",
    "$$f(X) = \\mathbb{P}(X \\in \\text{Class } 1), \\quad \\text{then} \\quad 1-f(X) = \\mathbb{P}(X \\in \\text{Class } 0).$$\n",
    "\n",
    "### Loss function and model fitting\n",
    "\n",
    "To train the model, we begin by treating the labels $0$ and $1$ as the *probabilities* of the input belonging to class $1$. Our objective then is to get our predictive probabilities $\\hat{y} = f(X)$ as close as possible to the target probabilities $y = 0$ or $1$, so for example, if the input data $X$ has label $y=1$, then we want our model to predict a *high probability* $f(X) \\approx 1$, and likewise if it has label $y=0$, then we want it to predict a *low probability* $f(X) \\approx 0$.\n",
    "\n",
    "One can show that this essentially boils down to finding a model parameter $\\boldsymbol{\\beta}^*$ that minimizes the **cross-entropy loss** (see [this article](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) for more details):\n",
    "\n",
    "$$L(Y, \\hat{Y}) = - \\left[Y \\log \\hat{Y} + (1 - Y) \\log(1 - \\hat{Y})\\right], \\quad \\text{where} \\quad \\hat{Y} = f(X; {\\boldsymbol \\beta})$$\n",
    "\n",
    "which measures the discrepancy between two probability distributions $Y$ and $\\hat{Y}$. Evaluating $L$ at the observed data $(X_n, y_n)$ and computing the average,\n",
    "$$\\mathcal{L}(\\boldsymbol{\\beta}) = \\frac1N \\sum_{n} L(y_n, f(X_n; {\\boldsymbol \\beta})),$$\n",
    "this becomes a function of $\\boldsymbol{\\beta}$ only, which can then be minimized using gradient-descent to obtain $\\boldsymbol \\beta^* = \\text{argmin}_{\\boldsymbol \\beta} \\, \\mathcal{L}(\\boldsymbol{\\beta})$.\n",
    "\n",
    "While it's good to know these details, all of this can be done automatically using the `scikit-learn` library in `python`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a good practice in machine learning is to split the dataset into a __training__ and __testing set__.\n",
    "\n",
    "![training-testing-set](https://docs.splunk.com/images/thumb/3/3b/TrainTest.png/550px-TrainTest.png)\n",
    "\n",
    "The training set is used to fit the model (i.e. determine the model parameters) and the testing set is only used at the very end to evaluate the performance of the model that was fit on the training set. By evaluating on the testing set which is not seen during training, we get an unbiased assessment of the model performance. Usually, training sets are chosen to be larger than the test sets such as an 80/20% split.\n",
    "\n",
    "__Tip:__ It is also a good practice to take a small chunk (approx. 10-20%) out of the training set called the *validation set* which can be used to select the best model and model configurations (i.e. tuning the model hyperparameters) before doing a final, final evaluation on the test set.\n",
    "\n",
    "In `scikit-learn`, splitting the dataset randomly into training and testing sets can be achieved easily using `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training (80%) and testing sets (20%)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we wish to fit the logistic model on the training data, which can be done easily using the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logistic model\n",
    "model_lr = LogisticRegression()\n",
    "\n",
    "# Fit model to training data\n",
    "model_lr.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the outputs of the model in `scikit-learn` is also very easy by using the `predict` method. Below, we plot the predictions of the model on the test dataset against the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(f, X, y, title):\n",
    "    ypred = f(X)\n",
    "    fig = plt.figure(figsize=(20, 4))\n",
    "    coord = {0: 'x', 1: 'y', 2: 'z'}\n",
    "    for i in range(3):\n",
    "        ax = plt.subplot(1, 3, i+1)\n",
    "        plt.plot(X[:250,i], y[:250], 'o', label='truth')\n",
    "        plt.plot(X[:250,i], ypred[:250], 'x', alpha=0.8, label='prediction')\n",
    "        ax.set_xlabel(coord[i], fontsize=15)\n",
    "        if i==0:\n",
    "            ax.set_ylabel('class', fontsize=15)\n",
    "    ax.legend(fontsize=15)\n",
    "    plt.suptitle(title, fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "plot_predictions(model_lr.predict, test_X, test_y, 'Predictions from Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the predictions from the fit model are not perfect and misclassifies many points. To quantify the percentage of points that the model classifies correctly, we can use the `score` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on the test set\n",
    "accuracy_lr = model_lr.score(test_X, test_y)\n",
    "print(\"Accuracy of logistic regression: {:.4f}\".format(accuracy_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result isn't too bad (at least it's better than chance), but there is definitely room for improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the feature vector with nonlinear features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to improve the model performance is by extending or reducing the number of *features* used for prediction. *Feature* is just a fancy name for the components of the input vector used to predict the output labels. For example in our case, the features that were used for prediciton were the $x, y$ and $z$ components of the points in the Lorenz attractor.\n",
    "\n",
    "However, we could have also considered a simpler model that makes predictions just based on the $x$ and $z$ components, or introduced a new feature $w = xy$ and made predictions based on the four-dimensional feature vector $(x, y, z, w)$.\n",
    "\n",
    "Manipulating and extending the features can improve the model when the raw inputs don't necessarily capture the information we need to fit the data. For example, if the data distribution has a quadratic trend, then clearly, augmenting the data with quadratic features will do better.\n",
    "On the other hand, reducing the number of features can be beneficial when there are too many features present, which often leads to *overfitting* (i.e., when a function fits closely to a limited set of data points but does not generalise well).\n",
    "\n",
    "In the following, we choose to *extend* the number of features using the nonlinear map $\\psi: \\mathbb{R}^3 \\rightarrow \\mathbb{R}^6$ (called the *feature map*), given by\n",
    "\n",
    "$$\\psi(x, y, z) = (x, y, z, xy, yz, xz)^T,$$\n",
    "\n",
    "thus augmenting the three components of L63 with extra quadratic interaction terms $xy$, $yz$ and $xz$. We can do this in `scikit-learn` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(2, interaction_only=True, include_bias=False)\n",
    "psi = poly.fit_transform\n",
    "\n",
    "# Extend features in training and testing set\n",
    "train_X_extended = psi(train_X)\n",
    "test_X_extended = psi(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fit a logistic regression model on this new training set with extended features, i.e. we fit a new model of the form\n",
    "\n",
    "$$f(x,y,z) = \\sigma( \\boldsymbol{\\beta}^T \\psi(x,y,z) ) = \\sigma(\\beta_0 + \\beta_1 x + \\beta_2 y + \\beta_3 z + \\beta_4 xy + \\beta_5 yz + \\beta_6 xz)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up new logistic regression model\n",
    "model_lr_plus = LogisticRegression()\n",
    "\n",
    "# Fit model on the extended training set\n",
    "model_lr_plus.fit(train_X_extended, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and evaluate this new model on the extended test set as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions and evaluate accuracy\n",
    "accuracy_lr_ext = model_lr_plus.score(test_X_extended, test_y)\n",
    "plot_predictions(model_lr_plus.predict, test_X_extended, test_y, 'Predictions from Logistic Regression+')\n",
    "print(\"Accuracy of logistic regression with extra quadratic features: {:.4f}\".format(accuracy_lr_ext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We definitely see an improvement over the vanilla logistic regression! This means that the quadratic interaction terms capture useful information to help us classify the points on the attractor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next popular method that we are going to investigate is *decision tree classification*. A decision tree is basically a type of look-up table that allow us to make decisions based on various conditions on the features. For example, let's say that there's an alien that arrived on earth for whatever reason that wants to know under what weather conditions they should go to the park without looking suspicious. By observing the humans in the city, they notice that there are three main attributes (features) that contribute to whether a person goes out to the park or not: precipitation, time and temperature. Based on the data they collected, they might form a decision tree like this:\n",
    "\n",
    "![Decision tree](../images/decisiontree.png)\n",
    "\n",
    "We'll omit the details on how to actually build a decision tree using the training data, but you can check out [this excellent video](https://www.youtube.com/watch?v=7VeUPuFGJHk) if you want to know more.\n",
    "In `scikit-learn`, building a decision tree can be done automatically using `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pretty much the same way as we have done with logistic regression, building a decision tree from the training data is extremely simple in `scikit-learn`, by using the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a decision tree model\n",
    "model_dt = DecisionTreeClassifier()\n",
    "\n",
    "# fit training data\n",
    "model_dt.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the tree that was just constructed and see how it classifies the points on the attractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision tree (up to depth 2)\n",
    "fig = plt.figure(figsize = (20, 10))\n",
    "sklearn.tree.plot_tree(model_dt,\n",
    "                       max_depth = 2,\n",
    "                       feature_names = ['x', 'y', 'z'],\n",
    "                       class_names = ['left', 'right'],\n",
    "                       filled = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming into a single node of the tree, we see that it contains several information:\n",
    "\n",
    "![decision tree node](../images/tree-node.png)\n",
    "\n",
    "The first line gives us the condition that it uses to classify the inputs further down the tree (here, $z \\leq 14.089$). If true, we follow the left arrow and if false, we follow the right arrow. Of all the inputs in the training set that are fed to this tree, the number of data that end up on this node is displayed in the third line (samples $= 3379$), and the fourth line indicates how many of these training samples belong to the class $0$ or $1$. So if value $= [a, b]$, this means that out of all the training data that end up on this node, there are $a$ samples that belong to class $0$ and $b$ samples that belong to class $1$. Based on this, the last line tells us the most likely class an arbitrary input belongs to if it ended up on that node, which is determined by whether $a > b$ (class $=0$) or $a < b$ (class $=1$).\n",
    "Finally, the *gini impurity*, indicated in the second line, measures how 'mixed' the training samples are at that node. This is computed as $g = \\frac{2ab}{(a+b)^2}$ so it reaches a minimum (zero) if either $a$ or $b$ is zero. If `max_depth` is not specified during training, decision trees will be trained until all the leaves have zero impurity.\n",
    "\n",
    "Now that we understand how decision tree works, let's go ahead and plot the predictions of the fitted model on the test data and compute the corresponding accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy of fitted tree on test data\n",
    "accuracy_dt = model_dt.score(test_X, test_y)\n",
    "\n",
    "# Plot predictions\n",
    "plot_predictions(model_dt.predict, test_X, test_y, 'Predictions from Decision Tree')\n",
    "\n",
    "# Print accuracy\n",
    "print(\"Accuracy of decision tree: {:.4f}\".format(accuracy_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is incredible! We see why decision trees are such popular class of models: it is easy to setup, easy to understand (interpretable) and usually does a great job at classifying things. Next, let's see if we can do better by making decisions based on a whole bunch of decision trees instead of just one, giving us a model that is appropriately called *random forests*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *random forest classification*, the main idea is to make the decision process a bit more 'democratic' by taking votes from a whole bunch of trees (a *forest* so to say) instead of letting a single tree dictate the decision. This prevents the model from making overconfident decisions, which results in less overfitting and therefore better results.\n",
    "For example, going back to the chihuahuas-and-muffins dataset from before, let's say that we are given an image of a muffin that really looks like a chihuahua (don't believe me?):\n",
    "\n",
    "![muffin that looks like a chihuahua](../images/chihuahuamuffin.png)\n",
    "\n",
    "A single decision tree might misclassify this as a chihuahua, but a random forest might get it right with slightly more votes for 'muffin':\n",
    "\n",
    "![how random forests vote](../images/chihuahua-muffin-vote.png)\n",
    "\n",
    "To construct the ensemble of trees in a random forest, we use a technique called *bagging*, which generates $m$ random training datasets by sampling (with replacement) from the original training set $m$ times. These new training datasets are then used to generate the trees in the forest.\n",
    "\n",
    "Again, this can be done automatically in `scikit-learn` using `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fit the model on the training data as before. Here, we restrict the depth of each tree to a maximum of 15 levels in order to save computation by passing the argument `max_depth=15`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up random forest model with a maximum depth of 15\n",
    "model_rf = RandomForestClassifier(max_depth=15)\n",
    "\n",
    "# Fit random forest on the training data\n",
    "model_rf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we visualise four trees in the forest that were generated and fitted on the training data. They all look slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 10))\n",
    "for i in range(4):\n",
    "    ax = plt.subplot(2, 2, i+1)\n",
    "    sklearn.tree.plot_tree(model_rf.estimators_[i],\n",
    "                           max_depth = 1,\n",
    "                           feature_names = ['x', 'y', 'z'],\n",
    "                           class_names = ['left', 'right'],\n",
    "                           filled = True)\n",
    "    ax.set_title(f'tree {i+1}', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the fitted random forest model performs on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(model_rf.predict, test_X, test_y, 'Predictions from Random Forest')\n",
    "accuracy_rf = model_rf.score(test_X, test_y)\n",
    "print(\"Accuracy of random forest: {:.4f}\".format(accuracy_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that this does a great job at classifying the points on the attractor, but we don't see a huge improvement over decision trees (it might even be slightly worse depending on the run). This really depends on the problem at hand and we may or may not see a huge improvement by using random forests. In this case, a single decision tree already does a good job and extending to random forests didn't improve much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we look at a class of models called *neural networks*, which has become very popular in the last ten or so years due to the increased availability of compute power needed to train them. Neural networks are basically an extremely powerful and flexible class of models that can capture highly complex, nonlinear relationships in data. Here, we'll only look at a certain class of neural networks called *multilayer perceptron (MLP)*, which has the following graphical structure:\n",
    "\n",
    "![multilayer perceptron](../images/neuralnet.png)\n",
    "\n",
    "To understand this graph, each node in the figure above, called a *unit*, contains a real number and these are connected by an arrow if the value at the tail depends on the value at the head according to some nonlinear function (we will see its particular form below). In an MLP, these units are stacked into several *layers* that is of one of three types: input, hidden or output. While there is only one each of input and output layers, there can be as many hidden layers as you want (not just two as the figure suggests). We say that the model is *deep* if there is more than one hidden layer, which is where the term *deep learning* comes from.\n",
    "\n",
    "To make predictions with neural networks, we first get a vector representation of the input $X$ by flattening it if necessary, which is then passed through the input, hidden and output layers as follows (we adopt the notation used in the figure above):\n",
    "\n",
    "\\begin{align*}\n",
    "(\\text{input layer}) \\qquad & \\quad x_i = flatten(X)_i \\\\ \\\\\n",
    "(\\text{hidden layers}) \\qquad\n",
    "&\n",
    "\\begin{cases}\n",
    "h_i^{(1)} = \\phi \\left(\\sum_j w_{ij}^{(1)} x_j + b_i^{(1)}\\right) \\\\\n",
    "h_i^{(2)} = \\phi \\left(\\sum_j w_{ij}^{(2)} h_j^{(1)} + b_i^{(2)}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "h_i^{(L)} = \\phi \\left(\\sum_j w_{ij}^{(L)} h_j^{(L-1)} + b_i^{(L)}\\right)\n",
    "\\end{cases}\n",
    "\\\\ \\\\\n",
    "(\\text{output layer}) \\qquad\n",
    "&\\quad o_i = \\sum_j w_{ij}^{out} h_j^{(L)} + b_i^{out}.\n",
    "\\end{align*}\n",
    "\n",
    "The function $\\phi : \\mathbb{R} \\rightarrow \\mathbb{R}$ that is applied in the hidden layers is called an *activation function* and they can be any nonlinear, continuous function.\n",
    "A popular choice for $\\phi$ in recent years is the ReLU function, which looks like this:\n",
    "\n",
    "![relu](https://miro.medium.com/max/357/1*oePAhrm74RNnNEolprmTaQ.png)\n",
    "\n",
    "This might seem arbitrary but is justified mathematically by the *universal approximation theorem*, which can roughly be stated as follows:\n",
    "\n",
    "**Theorem:** A multilayer perceptron with ReLU activation function can provide a piecewise linear approximation to *any* continuous function $f$ arbitrarily closely given enough layers or units. This result can also be extended to arbitrary activation funcitons under certain conditions.\n",
    "\n",
    "The parameters $(w_{ij}, b_i)$ of the neural network are called the *weights* and *biases* respectively and the objective of neural network modelling is to look for appropriate values of these parameters that best fit the data.\n",
    "\n",
    "### Classification with neural networks\n",
    "\n",
    "Classification using neural networks is done in the exact same way as classification using logistic regression, where the logits are now given by the outputs of a neural network, instead of the outputs of a linear model. That is, we first compute the probability\n",
    "\n",
    "$$\n",
    "f(X) = \\sigma(\\text{Neural network}(X)) \\in (0, 1),\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid or softmax function and then classify based on which class gets assigned the highest probability.\n",
    "The parameters of the model (the weights and biases) can be learned by optimizing the cross-entropy loss as we saw for logisitic regression using gradient descent or some variant of gradient descent ([Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) is usually the go-to optimizer for training deep networks). \n",
    "Neural network classification can therefore be interpreted as a type of logistic regression, where the feature map $\\psi$ is *learnt* automatically from data instead of being hand-crafted.\n",
    "\n",
    "In order to implement an MLP in `python`, we will use the `keras` library, which offers a simple interface for builiding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before sending an input data through the neural network, it is typically a good idea to apply re-scaling in order to increase its performance, since neural network often struggle when a variety of scales are present in the input features (see [this article](https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/) for more details).\n",
    "\n",
    "Let's look at the range of values that our input training data takes by using a box plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot(X: np.array, title: str):\n",
    "    # X must have size (n_samples, 3)\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    ax = fig.gca()\n",
    "    plt.boxplot(X, labels=('x', 'y', 'z'), showfliers=False)\n",
    "    ax.tick_params(axis='both', labelsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "box_plot(train_X, 'Distribution of training data before normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the inputs take a large range of values, with the $z$ component taking much higher values than the $x$ and $y$ components. To fix this, we apply normalization to the training data $X$ by first computing its mean $\\mu$ and variance $\\sigma^2$, then applying the transformation $\\hat{X} = (X - \\mu)/\\sigma$.\n",
    "\n",
    "Of course you can apply this transformation by hand, but here we will use the `Normalization` class in `keras`' `preprocessing` module, which does this automatically for us. A nice thing about using `Normalization` to normalize the inputs is that its class instance can be treated as just another layer in the neural network that can be stacked with the other layers, as we will see later.\n",
    "\n",
    "We use the `adapt` method to compute the mean and variance of the training data, which gets stored in the `mean` and `variance` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Normalization layer\n",
    "Normalize = Normalization()\n",
    "\n",
    "# Adapt to training data (i.e. compute its mean and variance)\n",
    "Normalize.adapt(train_X)\n",
    "\n",
    "# Print mean and variance\n",
    "print(f'mean of training data: {Normalize.mean.numpy()}')\n",
    "print(f'variance of training data: {Normalize.variance.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute a normalized training dataset $\\hat{X}$ by passing the training set $X$ through the `Normalize` object, which applies the transformation $\\hat{X} = (X - \\mu)/\\sigma$ using the values of mean and variance that it has stored earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize training data\n",
    "train_X_normalized = Normalize(train_X)\n",
    "\n",
    "# Plot distribution of normalized data\n",
    "box_plot(train_X_normalized.numpy(), 'Distribution of training data after normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data distribution looks much better now and we are ready to move on to the next step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Setting up a neural network with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to set up a multilayer pereptron in `keras`, we use the `Sequential` class, which build neural networks intuitively by simply stacking up the neural network layers one-by-one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = Sequential([\n",
    "    \n",
    "    # input layer\n",
    "    Input(shape=(3,)),\n",
    "    \n",
    "    # normalization\n",
    "    Normalize,\n",
    "    \n",
    "    # hidden layers\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    \n",
    "    # output layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this down step-by-step. First of all, we can see intuitively that we are just stacking up the neural network layers in order, as \"input layer -> normalizing layer -> hidden layers -> output layer\".\n",
    "\n",
    "The input layer in `keras` is modelled using the `Input` object and here, we have chosen to specify the shape of our input explicitly by passing the argument `shape=(3,)` to prevent bugs (if `shape` is not specified, this allows us to plug in input tensors of any shapes without raising errors).\n",
    "We can then either 1.) use the normalized training/test data as inputs, or 2.) use the unprocessed training/test data as inputs but apply normalization as part of the network architecture. Here, we choose to do the latter, which can done by stacking the `Normalize` layer that we just created right after the input layer.\n",
    "\n",
    "The hidden and output layers will be modelled in `keras` using the `Dense` object, which takes in the number of units and activation function as arguments, and implements the operation $y = \\phi(Wx + b)$, where $\\phi$ is the activation function that is specified (if activation is unspecified, we just get an affine transformation). \n",
    "Thus, we see that our network consists of $5$ hidden layers with ReLU activation, each with $100$ units, and an output layer of a single unit with sigmoid activation.\n",
    "\n",
    "Next, we specify the loss function and optimizer that we use to train our model, two essential ingredients aside from the network itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set loss function to be cross-entropy loss for binary classification\n",
    "loss_fn = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Use the Adam optimizer\n",
    "opt = keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and put all of these components together, using the `compile` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_nn.compile(loss=loss_fn,\n",
    "                 optimizer=opt,\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have specified `accuracy` as the metric with which we assess the model performance. Finally, we display the details of the network that we just put together using the `summary` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to fit our neural network on the training data. As we have done with logistic regression and decision trees, fitting a neural network in `keras` is super simple if we use the `fit` method (warning: you will get an error if you forget to compile the model with a loss function and an optimizer in the previous step). We also have to specify the number of *training epochs*, which is how many times the optimization algorithm cycles over the entire training dataset to minimize the loss. Here we choose `epochs=10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn.fit(train_X, train_y, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fitted our model on the training set, we want to plot the predictions on the test set.\n",
    "\n",
    "__Note:__ Our keras neural network only outputs the probability of belonging to class 1, so we first have to do a bit of fidgeting by wrapping our model with the class `to_binary`, which gives us a binary output of 0 or 1 depending on whether the output probability is less than 0.5 or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class to_binary():\n",
    "    def __init__(self, model, cutoff=0.5):\n",
    "        self.model = model\n",
    "        self.cutoff = cutoff\n",
    "    def predict(self, X):\n",
    "        ypred = self.model(X)\n",
    "        ypred_binary = np.array([0 if p < self.cutoff else 1 for p in ypred])\n",
    "        return ypred_binary\n",
    "\n",
    "model_nn_binary = to_binary(model_nn)\n",
    "plot_predictions(model_nn_binary.predict, test_X, test_y, 'Predictions from Neural Network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good! Now let's get a quantitative assessment of the performance on the test set using the `evaluate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_nn, accuracy_nn = model_nn.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the neural network does significantly better than logistic regression (both the vanilla version and the version with extended features), however it does not out-perform the decision tree/random forest classification. Hence, the winner of this task is decision trees/random forests!\n",
    "\n",
    "__Tip:__ Experiment with different numbers of hidden layers, units, number of epochs, etc in the neural network and see how it affects the test accuracy. Also observe what happens when you remove the normalization layer from the network. You should see that the performance becomes slightly worse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
